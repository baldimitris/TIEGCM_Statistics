<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>TIEGCM_Statistics.Manager API documentation</title>
<meta name="description" content="This module executes the statistical calculation.
It reads all the TIE-GCM NetCDF files and for each of them creates a computer process to process â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TIEGCM_Statistics.Manager</code></h1>
</header>
<section id="section-intro">
<p>This module executes the statistical calculation.<br>
It reads all the TIE-GCM NetCDF files and for each of them creates a computer process to process it.<br>
Each process creates temporary binary files containing the values which fall into each bin.<br>
After all processes are finished the module calculates percentiles for each bin.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This module executes the statistical calculation.  
It reads all the TIE-GCM NetCDF files and for each of them creates a computer process to process it.  
Each process creates temporary binary files containing the values which fall into each bin.  
After all processes are finished the module calculates percentiles for each bin.
&#34;&#34;&#34;

# local imports
import Data

# system imports
import netCDF4
from netCDF4 import Dataset 
import os
import datetime
import time
import glob
import shutil
import math
import numpy as np
import multiprocessing
from pathlib import Path
import random
from array import array

NUM_OF_PROCESSORS = 14
TMP_FOLDER =  &#34;./results/tmp/&#34;  &#34;/media/balukid/STATStmp/&#34;
USE_WEIGHTED_AVERAGE = False

global theResultFile_folder
global theResultFile_simplename

def StartCalculating( NetCDF_files_path, ResultFilename, TypeOfCalculation, TmpFilesPath=&#34;/media/balukid/STATStmp/&#34; ):
    &#39;&#39;&#39;
    This function manages the statistical calulation:  
      - For each TIE-GCM netCDF file it spawns a process which will store its results into temporary files at its own folder. Many procecess are utilised in order to accelerate the calculation by leveraging multiple cores.  
      - After all processes finish, this function merges all temporary files, calculates percentiles etc for each bin and creates a results netCDF file.  
    This function should be called after the Data.setDataParams() function has initialized the bin ranges.
    Args:
        NetCDF_files_path (string): The path where all the TIE-GCM netCDF files are stored. In wildcard format. Example: &#34;./data/*/*.nc&#34;.
        ResultFilename (string): The filename where the final calculation results will be stored.
        TypeOfCalculation (string): The variable which upon which the calculation will be applied. See Data.setDataParams() for more.
        TmpFilesPath (string): The path where the temporary files will be stores
    &#39;&#39;&#39;
    global TMP_FOLDER
    global theResultFile_folder
    global theResultFile_simplename
    theResultFile_folder   = ResultFilename[ 0 : ResultFilename.rfind(&#39;/&#39;)  ]
    theResultFile_simplename = ResultFilename[ ResultFilename.rfind(&#39;/&#39;)+1 :  ][0:-3]
    if not os.path.exists(theResultFile_folder+&#34;/&#34;+theResultFile_simplename): os.makedirs(theResultFile_folder+&#34;/&#34;+theResultFile_simplename)
    
    startSecs = time.time()
    print( &#34;START&#34;, datetime.datetime.now().strftime(&#34;%d-%m-%Y %H:%M:%S&#34;) )
    TMP_FOLDER = TmpFilesPath
    
    Allprocesses = list()
    AllCDFfiles = sorted( glob.glob( NetCDF_files_path, recursive=True ) )
    print( &#34;I will calculate &#39;&#34; + TypeOfCalculation + &#34;&#39; on&#34;, len(AllCDFfiles), &#34;files:\n    &#34;, NetCDF_files_path, &#34;\n&#34; )
    print( &#34;Results will be stored in &#39;&#34; + ResultFilename + &#34;&#39;\n&#34; )
    
    n = 0
    for CDF_file in AllCDFfiles:
        n += 1
        Data.Progress = int( 100 * n/221)

        # spawn new process
        P = multiprocessing.Process(target=PROC_StatsCalculator, args=(n,CDF_file,TypeOfCalculation))
        Allprocesses.append(P)
        P.start()

        pause_spawning = True
        while pause_spawning:
            Num_of_alive_processes = 0
            for P in Allprocesses:
                if P.is_alive():
                    Num_of_alive_processes += 1            
            if Num_of_alive_processes &gt;= NUM_OF_PROCESSORS:
                pause_spawning = True
                time.sleep(12)
            else:
                pause_spawning = False


        # wait for all processes to terminate
        for T in Allprocesses: T.join()
        
    # every process creates a partial file, merge all of them into one
    print( &#34;Merging partial data files and calculating result values...&#34;,  datetime.datetime.now().strftime(&#34;%d-%m-%Y %H:%M:%S&#34;))
    ResultBuckets = Data.init_ResultDataStructure()
    NumOfBins = len(Data.KPsequence) * len(Data.ALTsequence) * len(Data.LATsequence) * len(Data.MLTsequence)
    CurrBinNum = 0
    
    print( &#34;Data.KPsequence: &#34;, Data.KPsequence)
    print( &#34;Data.ALTsequence&#34;, Data.ALTsequence)
    print( &#34;Data.LATsequence&#34;, Data.LATsequence)
    print( &#34;Data.MLTsequence&#34;, Data.MLTsequence)
    
    for aKP in Data.KPsequence:
        for aMLT in Data.MLTsequence:
            RegionHits = 0
            for anALT in Data.ALTsequence:
                for aLat in Data.LATsequence:
                    CurrBinNum += 1
                    Data.Progress = int( 100 * CurrBinNum/NumOfBins )
                    AllBinValues = list()
                    All_partialData_folders = sorted( glob.glob( TMP_FOLDER+&#34;proc*&#34;, recursive=False ) )
                    for partialDataFolder in All_partialData_folders: # read all partial files for this bin 
                        partialDataFolder = partialDataFolder + &#34;/&#34;
                        if os.path.isdir(partialDataFolder)==False:
                            continue
                        partialDataFilename = partialDataFolder + str(aKP)+&#34;_&#34;+str(anALT)+&#34;_&#34;+str(aLat)+&#34;_&#34;+str(aMLT)+&#34;.dat&#34;
                        if os.path.exists(partialDataFilename) == False: # no hits for this bin from this process
                            continue
                            
                        f = open(partialDataFilename, &#34;rb&#34;)
                        float_array = array(&#39;d&#39;)
                        float_array.frombytes(f.read())
                        AllBinValues += float_array.tolist()
                        f.close()
                        
                    print(&#34;BIN&#34;, &#34;Kp&#34;+str(aKP), &#34;Alt&#34;+str(anALT), &#34;Lat&#34;+str(aLat), &#34;MLT&#34;+str(aMLT), &#34;&#34;, len(AllBinValues), &#34;items&#34; )
                    RegionHits += len(AllBinValues)
                        
                    if len(AllBinValues) &gt; 0:
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Sum&#34;] = np.sum(AllBinValues)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Len&#34;] = len(AllBinValues)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile10&#34;] = np.percentile(AllBinValues, 10)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile25&#34;] = np.percentile(AllBinValues, 25)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile50&#34;] = np.percentile(AllBinValues, 50)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile75&#34;] = np.percentile(AllBinValues, 75)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile90&#34;] = np.percentile(AllBinValues, 90)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Variance&#34;] = np.var(AllBinValues)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Minimum&#34;] = np.nanmin(AllBinValues)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Maximum&#34;] = np.nanmax(AllBinValues)
                        
                        # calculate distribution
                        if Data.DistributionNumOfSlots &gt; 0:
                            histo_values, histo_ranges = np.histogram(AllBinValues, Data.DistributionNumOfSlots, (0, 0.0000001))
                            for i in range(0, Data.DistributionNumOfSlots):
                                ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Distribution&#34;][i] = histo_values[i]
            print( &#34;REGION:&#34;, &#34;Kp&#34;+str(aKP), &#34;MLT&#34;+str(aMLT), &#34;:&#34;, RegionHits, &#34;measurements.&#34; )
            print(&#34;BIN&#34;, &#34;Kp&#34;+str(aKP), &#34;Alt&#34;+str(anALT), &#34;Lat&#34;+str(aLat), &#34;MLT&#34;+str(aMLT), &#34;&#34;, len(AllBinValues), &#34;items&#34; )
            
    if &#34;Ohmic&#34; in TypeOfCalculation:
        Data.WriteResultsToCDF(ResultBuckets, ResultFilename, &#34;Joule Heating&#34;, &#34;W/m3&#34;)
    elif &#34;SIGMA_PED&#34; in TypeOfCalculation:
        Data.WriteResultsToCDF(ResultBuckets, ResultFilename, &#34;Pedersen Conductivity&#34;, &#34;S/m&#34;)
    else:
        Data.WriteResultsToCDF(ResultBuckets, ResultFilename, TypeOfCalculation, &#34;&#34;)
    
    # DO NOT DELL PARTIAL FILES - THEY CAN BE USED TO CONTINUE THE CALCULATION AFTER AN INTERMEDIATE HALT
    #try: # delete temporary files, which contain all values for each bin
    #    shutil.rmtree( TMP_FOLDER )
    #except:
    #    pass
    
    finishSecs = time.time()
    print( &#34;FINISH&#34;,  datetime.datetime.now().strftime(&#34;%d-%m-%Y %H:%M:%S&#34;), &#34; (&#34;, finishSecs-startSecs, &#34;sec )&#34;)

                  
                  
    

def PROC_StatsCalculator(ProcessNum, CDF_filename, TypeOfCalculation):
    &#39;&#39;&#39;
    Reads a NetCDF file and separates all the values of the variable into different files according to the bin they fall in.  
    The variable is chosen by the &lt;TypeOfCalculation&gt; argument.  
    The process saves several files in its own folder with the name: TMP_FOLDER+&#34;proc&#34;+&lt;ProcessNum&gt;+&#34;/&#34;.  
    The folder contains one binary file for each bin. The file contains all values of the variable which fall inside the bin.
    The function also saves into the result files the Number Of Measurements per bin it has processed.
    Args:
        ProcessNum (int):
        CDF_filename (string):
        TypeOfCalculation (string):
    &#39;&#39;&#39;
    # check if the data of this process have already been calculated
    procfolder = TMP_FOLDER+&#34;proc&#34;+ f&#34;{ProcessNum:03}&#34; +&#34;/&#34;
    if os.path.isdir(procfolder):
        print( &#34;Proc &#34;, ProcessNum, &#34;already calculated.&#34;, flush=True )
        return # &lt;&lt;&lt;&lt;
    else:
        if os.path.isdir(TMP_FOLDER)==False: os.mkdir( TMP_FOLDER )
        os.mkdir( procfolder )    
    
    # open netCDF file 
    print(&#34;Proc&#34;,ProcessNum,&#34;reading &#34;,CDF_filename[CDF_filename.rfind(&#39;/&#39;)+1:], datetime.datetime.now().strftime(&#34;%d-%m-%Y %H:%M:%S&#34;), flush=True)
    try:
        CDFroot = Dataset( CDF_filename, &#39;r&#39; )
    except:
        print ( &#34; !!! WRONG FORMAT:&#34;, CDF_filename, flush=True )
        #os.remove(&#34;ReadingFile.flag&#34;) # lower the reading-file flag
        return
        
    # read the data from the netCDF file
    #TIMEs  = CDFroot.variables[&#39;time&#39;][:] 
    if &#34;Ohmic&#34; in TypeOfCalculation or &#34;JH&#34; in TypeOfCalculation:        Ohmics = CDFroot.variables[&#39;Ohmic&#39;][:, :, :, :]  # m/s
    if &#34;SIGMA_PED&#34; in TypeOfCalculation:   PEDs   = CDFroot.variables[&#39;SIGMA_PED&#39;][:, :, :, :]
    if &#34;SIGMA_HAL&#34; in TypeOfCalculation:  HALs   = CDFroot.variables[&#39;SIGMA_HAL&#39;][:, :, :, :]
    if &#34;EEX_si&#34; in TypeOfCalculation:    EEXs   = CDFroot.variables[&#39;EEX_si&#39;][:, :, :, :]
    if &#34;EEY_si&#34; in TypeOfCalculation:    EEYs   = CDFroot.variables[&#39;EEY_si&#39;][:, :, :, :]
    if &#34;Convection_heating&#34; in TypeOfCalculation:  ConvH  = CDFroot.variables[&#39;Convection_heating&#39;][:, :, :, :]
    if &#34;Wind_heating&#34; in TypeOfCalculation:  WindH  = CDFroot.variables[&#39;Wind_heating&#39;][:, :, :, :]
    if &#34;JHminusWindHeat&#34; in TypeOfCalculation:  WindH  = CDFroot.variables[&#39;Wind_heating&#39;][:, :, :, :]        
        
    #
    LATs   = CDFroot.variables[&#39;lat&#39;][:] 
    #MLATs   = CDFroot.variables[&#39;mlat_qdf&#39;][:, :, :, :] 
    MLTs    = CDFroot.variables[&#39;mlt_qdf&#39;][:, :, :, :]         
    ALTs    = CDFroot.variables[&#39;ZGMID&#39;][:, :, :, :] / 100000 # Geometric height stored in cm, converted to km
    KPs     = CDFroot.variables[&#39;Kp&#39;][:]
    
    hits = 0   # num of instances that fit in any of the defined bins

    ResultBuckets = Data.init_ResultDataStructure().copy()
    num_of_unbinned_items = 0
    step = 1
    for idx_time in range(0, len(ALTs), step):
        # $$$$$$$$ for each moment in time put the values in their bins and calculate the mean of each bin. 
        SingleMomentBuckets = Data.init_ResultDataStructure().copy()
        for idx_lev in range(0, len(ALTs[0]), step):
            for idx_lat in range(0, len(ALTs[0,0]), step):
                for idx_lon in range(0, len(ALTs[0,0,0]), step):
                    
                    curr_alt_km = ALTs[idx_time, idx_lev, idx_lat, idx_lon] 
                    
                    # ignore values for out-of-range positions 
                    if curr_alt_km&lt;Data.ALT_min or curr_alt_km&gt;Data.ALT_max:
                        continue
                        
                    curr_kp     = KPs[idx_time]
                    curr_mlt    = MLTs[idx_time, idx_lev, idx_lat, idx_lon]
                    curr_lat    = LATs[idx_lat]
                    
                    kp_to_fall,alt_to_fall,lat_to_fall,mlt_to_fall = Data.LocatePositionInBuckets(curr_kp,curr_alt_km,curr_lat,curr_mlt)
                    
                    if kp_to_fall is None or alt_to_fall is None or lat_to_fall is None or mlt_to_fall is None:
                        num_of_unbinned_items += 1
                        break # no other longitude can have a hit either
                    else:
                        if TypeOfCalculation==&#34;JHminusWindHeat&#34; or TypeOfCalculation==&#34;JHminusWindHeatEISCAT&#34;:
                            aValue = Ohmics[idx_time, idx_lev, idx_lat, idx_lon] - WindH[idx_time, idx_lev, idx_lat, idx_lon]
                            if aValue &gt; 100: continue # ignore faulty large values
                        elif TypeOfCalculation==&#34;Ohmic&#34;:
                            aValue = Ohmics[idx_time, idx_lev, idx_lat, idx_lon]
                            if aValue &gt; 100: continue # ignore faulty large values
                        elif &#34;SIGMA_PED&#34; in TypeOfCalculation:
                            aValue = PEDs[idx_time, idx_lev, idx_lat, idx_lon]
                        elif &#34;HallCond&#34; in TypeOfCalculation:
                            aValue = HALs[idx_time, idx_lev, idx_lat, idx_lon]
                        elif &#34;EEX_si&#34; in TypeOfCalculation:
                            aValue = EEXs[idx_time, idx_lev, idx_lat, idx_lon]
                        elif &#34;EEY_si&#34; in TypeOfCalculation:
                            aValue = EEYs[idx_time, idx_lev, idx_lat, idx_lon]
                        elif &#34;Convection_heating&#34; in TypeOfCalculation:
                            aValue = ConvH[idx_time, idx_lev, idx_lat, idx_lon]
                            if aValue &gt; 100: continue # ignore faulty large values
                        elif &#34;Wind_heating&#34; in TypeOfCalculation:
                            aValue = WindH[idx_time, idx_lev, idx_lat, idx_lon]
                        else:
                            print(&#34;ERROR: UNRECOGNISED TypeOfCalculation &#39;&#34; + TypeOfCalculation + &#34;&#39;&#34;)
                            CDFroot.close()
                            return
                        
                        # bin this value
                        SingleMomentBuckets[ kp_to_fall, alt_to_fall, lat_to_fall, mlt_to_fall, &#34;Vals&#34; ].append( aValue )
                        
                        # if weights are enabled then store the value&#39;s weight as well
                        &#39;&#39;&#39;
                        TIEGCM latitudes are: 68.75,  71.25,  73.75,  76.25,  78.75
                        https://en.wikipedia.org/wiki/Spherical_segment
                        Area of a sphere segment = 2 * pi * R * height of the segment (the distance from one parallel plane to the other) 
                        R = 6477km
                        sin(angleFrom) = h_from / R
                        sin(angleTo)   = h_to   / R
                        &#39;&#39;&#39;
                        if USE_WEIGHTED_AVERAGE:
                            weight = 1
                            if curr_lat == 68.75:
                                weight = 0.0410
                            elif curr_lat == 71.25:
                                weight = 0.381
                            elif curr_lat == 73.75:
                                weight = 0.328
                            elif curr_lat == 76.25:
                                weight = 0.249
                            else:
                                print( &#34;&gt;&gt;&gt;&gt; &#34;, curr_lat )
                            SingleMomentBuckets[ kp_to_fall, alt_to_fall, lat_to_fall, mlt_to_fall, &#34;Weights&#34; ].append( weight )
                        
                        # keep tracks of the number of the total binned values 
                        hits +=1
                        
        # $$$$$$$$ the averages of each time moment are stored in their bin. The percentiles will be calculated on them at the end 
        if USE_WEIGHTED_AVERAGE: # weighted average
            for aKP in Data.KPsequence:
                for anALT in Data.ALTsequence:
                    for aLat in Data.LATsequence:
                        for aMLT in Data.MLTsequence: 
                            L = len(SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])
                            if L &gt; 0:
                                S = 0
                                sum_of_weights = 0
                                BinVals    = SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;]
                                BinWeights = SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Weights&#34;]
                                for i in range(0, len(SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])):
                                    S +=  BinWeights[i] * BinVals[i]
                                    sum_of_weights += BinWeights[i]
                                ResultBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;].append( S / sum_of_weights )
                                
        else: # normal average
            for aKP in Data.KPsequence:
                for aMLT in Data.MLTsequence: 
                    subfigure_N = 0
                    for aLat in Data.LATsequence:
                        for anALT in Data.ALTsequence:
                            L = len(SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])
                            if L &gt; 0:
                                S = sum(SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])
                                ResultBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;].append( S / L )
                                subfigure_N += L
                                
                    if subfigure_N &gt; 0: Store_NumOfMeasurements(aKP,aMLT, subfigure_N)

    # close cdf
    CDFroot.close()
    
    # ---- save results
    # save values of each bin in a binary file
    for aKP in Data.KPsequence:
        for anALT in Data.ALTsequence:
            for aLat in Data.LATsequence:
                for aMLT in Data.MLTsequence:    
                    if len( ResultBuckets[ aKP, anALT, aLat, aMLT, &#34;Vals&#34; ] ) &gt; 0:
                        fname = str(aKP) + &#34;_&#34; + str(anALT) + &#34;_&#34; + str(aLat) + &#34;_&#34; + str(aMLT) + &#34;.dat&#34;
                        f = open( procfolder + fname, &#34;wb&#34; )
                        float_array = array(&#39;d&#39;, ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Vals&#34;])
                        float_array.tofile(f)
                        f.close()

    # -------- print result message
    msg = &#34;Proc &#34; + str(ProcessNum) + &#34; &#34; + CDF_filename[-20:] +  &#34; Hits=&#34; + str(hits)
    for aKP in Data.KPsequence:
        msg += &#34;\n&#34;
        for aMLT in Data.MLTsequence: 
            n = 0
            for aLat in Data.LATsequence:
                for anALT in Data.ALTsequence:
                    n += len(ResultBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])
            msg += &#34; &#34; + str(n)
    msg += &#34;    &#34; + datetime.datetime.now().strftime(&#34;%H:%M:%S&#34;) + &#34;\n&#34;
    print(msg, flush=True)
    
    
    
    
def Store_NumOfMeasurements(aKP, aMLT, curr_num):
    &#39;&#39;&#39;
    Stores the number of measurements that fall inside each sub-region defined by its lower Kp limit and lower MLT limit. 
    There is one file per sub-region.
    This function is necessary because the data are processed by different processes.
    Args:
        aKP (float):
        aMLT (float):
        curr_num (int):
    &#39;&#39;&#39;
    # LOCK
    wait = True
    while( wait ):
        if os.path.exists( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + &#34;lock.tmp&#34; ) == False: 
            try:
                f = open( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + &#34;lock.tmp&#34;, &#34;x&#34; )
                f.close()
                wait = False
            except:
                wait = True
        if wait: time.sleep( random.randint(0,1) )
    # WORK
    prev_num = 0
    fname = str(aKP) + &#34;_&#34; + str(aMLT) + &#34;.txt&#34;
    if os.path.exists( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + fname ):
        f = open( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + fname, &#34;r&#34; )
        prev_num = int( f.read() )
        f.close()
    f = open( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + fname, &#34;w&#34; )
    f.write( str(prev_num + curr_num) )
    f.close()
    # UNLOCK
    try:
        os.remove( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + &#34;lock.tmp&#34; )
    except:
        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="TIEGCM_Statistics.Manager.PROC_StatsCalculator"><code class="name flex">
<span>def <span class="ident">PROC_StatsCalculator</span></span>(<span>ProcessNum, CDF_filename, TypeOfCalculation)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a NetCDF file and separates all the values of the variable into different files according to the bin they fall in.<br>
The variable is chosen by the <TypeOfCalculation> argument.<br>
The process saves several files in its own folder with the name: TMP_FOLDER+"proc"+<ProcessNum>+"/".<br>
The folder contains one binary file for each bin. The file contains all values of the variable which fall inside the bin.
The function also saves into the result files the Number Of Measurements per bin it has processed.</p>
<h2 id="args">Args</h2>
<p>ProcessNum (int):
CDF_filename (string):
TypeOfCalculation (string):</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def PROC_StatsCalculator(ProcessNum, CDF_filename, TypeOfCalculation):
    &#39;&#39;&#39;
    Reads a NetCDF file and separates all the values of the variable into different files according to the bin they fall in.  
    The variable is chosen by the &lt;TypeOfCalculation&gt; argument.  
    The process saves several files in its own folder with the name: TMP_FOLDER+&#34;proc&#34;+&lt;ProcessNum&gt;+&#34;/&#34;.  
    The folder contains one binary file for each bin. The file contains all values of the variable which fall inside the bin.
    The function also saves into the result files the Number Of Measurements per bin it has processed.
    Args:
        ProcessNum (int):
        CDF_filename (string):
        TypeOfCalculation (string):
    &#39;&#39;&#39;
    # check if the data of this process have already been calculated
    procfolder = TMP_FOLDER+&#34;proc&#34;+ f&#34;{ProcessNum:03}&#34; +&#34;/&#34;
    if os.path.isdir(procfolder):
        print( &#34;Proc &#34;, ProcessNum, &#34;already calculated.&#34;, flush=True )
        return # &lt;&lt;&lt;&lt;
    else:
        if os.path.isdir(TMP_FOLDER)==False: os.mkdir( TMP_FOLDER )
        os.mkdir( procfolder )    
    
    # open netCDF file 
    print(&#34;Proc&#34;,ProcessNum,&#34;reading &#34;,CDF_filename[CDF_filename.rfind(&#39;/&#39;)+1:], datetime.datetime.now().strftime(&#34;%d-%m-%Y %H:%M:%S&#34;), flush=True)
    try:
        CDFroot = Dataset( CDF_filename, &#39;r&#39; )
    except:
        print ( &#34; !!! WRONG FORMAT:&#34;, CDF_filename, flush=True )
        #os.remove(&#34;ReadingFile.flag&#34;) # lower the reading-file flag
        return
        
    # read the data from the netCDF file
    #TIMEs  = CDFroot.variables[&#39;time&#39;][:] 
    if &#34;Ohmic&#34; in TypeOfCalculation or &#34;JH&#34; in TypeOfCalculation:        Ohmics = CDFroot.variables[&#39;Ohmic&#39;][:, :, :, :]  # m/s
    if &#34;SIGMA_PED&#34; in TypeOfCalculation:   PEDs   = CDFroot.variables[&#39;SIGMA_PED&#39;][:, :, :, :]
    if &#34;SIGMA_HAL&#34; in TypeOfCalculation:  HALs   = CDFroot.variables[&#39;SIGMA_HAL&#39;][:, :, :, :]
    if &#34;EEX_si&#34; in TypeOfCalculation:    EEXs   = CDFroot.variables[&#39;EEX_si&#39;][:, :, :, :]
    if &#34;EEY_si&#34; in TypeOfCalculation:    EEYs   = CDFroot.variables[&#39;EEY_si&#39;][:, :, :, :]
    if &#34;Convection_heating&#34; in TypeOfCalculation:  ConvH  = CDFroot.variables[&#39;Convection_heating&#39;][:, :, :, :]
    if &#34;Wind_heating&#34; in TypeOfCalculation:  WindH  = CDFroot.variables[&#39;Wind_heating&#39;][:, :, :, :]
    if &#34;JHminusWindHeat&#34; in TypeOfCalculation:  WindH  = CDFroot.variables[&#39;Wind_heating&#39;][:, :, :, :]        
        
    #
    LATs   = CDFroot.variables[&#39;lat&#39;][:] 
    #MLATs   = CDFroot.variables[&#39;mlat_qdf&#39;][:, :, :, :] 
    MLTs    = CDFroot.variables[&#39;mlt_qdf&#39;][:, :, :, :]         
    ALTs    = CDFroot.variables[&#39;ZGMID&#39;][:, :, :, :] / 100000 # Geometric height stored in cm, converted to km
    KPs     = CDFroot.variables[&#39;Kp&#39;][:]
    
    hits = 0   # num of instances that fit in any of the defined bins

    ResultBuckets = Data.init_ResultDataStructure().copy()
    num_of_unbinned_items = 0
    step = 1
    for idx_time in range(0, len(ALTs), step):
        # $$$$$$$$ for each moment in time put the values in their bins and calculate the mean of each bin. 
        SingleMomentBuckets = Data.init_ResultDataStructure().copy()
        for idx_lev in range(0, len(ALTs[0]), step):
            for idx_lat in range(0, len(ALTs[0,0]), step):
                for idx_lon in range(0, len(ALTs[0,0,0]), step):
                    
                    curr_alt_km = ALTs[idx_time, idx_lev, idx_lat, idx_lon] 
                    
                    # ignore values for out-of-range positions 
                    if curr_alt_km&lt;Data.ALT_min or curr_alt_km&gt;Data.ALT_max:
                        continue
                        
                    curr_kp     = KPs[idx_time]
                    curr_mlt    = MLTs[idx_time, idx_lev, idx_lat, idx_lon]
                    curr_lat    = LATs[idx_lat]
                    
                    kp_to_fall,alt_to_fall,lat_to_fall,mlt_to_fall = Data.LocatePositionInBuckets(curr_kp,curr_alt_km,curr_lat,curr_mlt)
                    
                    if kp_to_fall is None or alt_to_fall is None or lat_to_fall is None or mlt_to_fall is None:
                        num_of_unbinned_items += 1
                        break # no other longitude can have a hit either
                    else:
                        if TypeOfCalculation==&#34;JHminusWindHeat&#34; or TypeOfCalculation==&#34;JHminusWindHeatEISCAT&#34;:
                            aValue = Ohmics[idx_time, idx_lev, idx_lat, idx_lon] - WindH[idx_time, idx_lev, idx_lat, idx_lon]
                            if aValue &gt; 100: continue # ignore faulty large values
                        elif TypeOfCalculation==&#34;Ohmic&#34;:
                            aValue = Ohmics[idx_time, idx_lev, idx_lat, idx_lon]
                            if aValue &gt; 100: continue # ignore faulty large values
                        elif &#34;SIGMA_PED&#34; in TypeOfCalculation:
                            aValue = PEDs[idx_time, idx_lev, idx_lat, idx_lon]
                        elif &#34;HallCond&#34; in TypeOfCalculation:
                            aValue = HALs[idx_time, idx_lev, idx_lat, idx_lon]
                        elif &#34;EEX_si&#34; in TypeOfCalculation:
                            aValue = EEXs[idx_time, idx_lev, idx_lat, idx_lon]
                        elif &#34;EEY_si&#34; in TypeOfCalculation:
                            aValue = EEYs[idx_time, idx_lev, idx_lat, idx_lon]
                        elif &#34;Convection_heating&#34; in TypeOfCalculation:
                            aValue = ConvH[idx_time, idx_lev, idx_lat, idx_lon]
                            if aValue &gt; 100: continue # ignore faulty large values
                        elif &#34;Wind_heating&#34; in TypeOfCalculation:
                            aValue = WindH[idx_time, idx_lev, idx_lat, idx_lon]
                        else:
                            print(&#34;ERROR: UNRECOGNISED TypeOfCalculation &#39;&#34; + TypeOfCalculation + &#34;&#39;&#34;)
                            CDFroot.close()
                            return
                        
                        # bin this value
                        SingleMomentBuckets[ kp_to_fall, alt_to_fall, lat_to_fall, mlt_to_fall, &#34;Vals&#34; ].append( aValue )
                        
                        # if weights are enabled then store the value&#39;s weight as well
                        &#39;&#39;&#39;
                        TIEGCM latitudes are: 68.75,  71.25,  73.75,  76.25,  78.75
                        https://en.wikipedia.org/wiki/Spherical_segment
                        Area of a sphere segment = 2 * pi * R * height of the segment (the distance from one parallel plane to the other) 
                        R = 6477km
                        sin(angleFrom) = h_from / R
                        sin(angleTo)   = h_to   / R
                        &#39;&#39;&#39;
                        if USE_WEIGHTED_AVERAGE:
                            weight = 1
                            if curr_lat == 68.75:
                                weight = 0.0410
                            elif curr_lat == 71.25:
                                weight = 0.381
                            elif curr_lat == 73.75:
                                weight = 0.328
                            elif curr_lat == 76.25:
                                weight = 0.249
                            else:
                                print( &#34;&gt;&gt;&gt;&gt; &#34;, curr_lat )
                            SingleMomentBuckets[ kp_to_fall, alt_to_fall, lat_to_fall, mlt_to_fall, &#34;Weights&#34; ].append( weight )
                        
                        # keep tracks of the number of the total binned values 
                        hits +=1
                        
        # $$$$$$$$ the averages of each time moment are stored in their bin. The percentiles will be calculated on them at the end 
        if USE_WEIGHTED_AVERAGE: # weighted average
            for aKP in Data.KPsequence:
                for anALT in Data.ALTsequence:
                    for aLat in Data.LATsequence:
                        for aMLT in Data.MLTsequence: 
                            L = len(SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])
                            if L &gt; 0:
                                S = 0
                                sum_of_weights = 0
                                BinVals    = SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;]
                                BinWeights = SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Weights&#34;]
                                for i in range(0, len(SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])):
                                    S +=  BinWeights[i] * BinVals[i]
                                    sum_of_weights += BinWeights[i]
                                ResultBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;].append( S / sum_of_weights )
                                
        else: # normal average
            for aKP in Data.KPsequence:
                for aMLT in Data.MLTsequence: 
                    subfigure_N = 0
                    for aLat in Data.LATsequence:
                        for anALT in Data.ALTsequence:
                            L = len(SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])
                            if L &gt; 0:
                                S = sum(SingleMomentBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])
                                ResultBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;].append( S / L )
                                subfigure_N += L
                                
                    if subfigure_N &gt; 0: Store_NumOfMeasurements(aKP,aMLT, subfigure_N)

    # close cdf
    CDFroot.close()
    
    # ---- save results
    # save values of each bin in a binary file
    for aKP in Data.KPsequence:
        for anALT in Data.ALTsequence:
            for aLat in Data.LATsequence:
                for aMLT in Data.MLTsequence:    
                    if len( ResultBuckets[ aKP, anALT, aLat, aMLT, &#34;Vals&#34; ] ) &gt; 0:
                        fname = str(aKP) + &#34;_&#34; + str(anALT) + &#34;_&#34; + str(aLat) + &#34;_&#34; + str(aMLT) + &#34;.dat&#34;
                        f = open( procfolder + fname, &#34;wb&#34; )
                        float_array = array(&#39;d&#39;, ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Vals&#34;])
                        float_array.tofile(f)
                        f.close()

    # -------- print result message
    msg = &#34;Proc &#34; + str(ProcessNum) + &#34; &#34; + CDF_filename[-20:] +  &#34; Hits=&#34; + str(hits)
    for aKP in Data.KPsequence:
        msg += &#34;\n&#34;
        for aMLT in Data.MLTsequence: 
            n = 0
            for aLat in Data.LATsequence:
                for anALT in Data.ALTsequence:
                    n += len(ResultBuckets[aKP,anALT,aLat,aMLT,&#34;Vals&#34;])
            msg += &#34; &#34; + str(n)
    msg += &#34;    &#34; + datetime.datetime.now().strftime(&#34;%H:%M:%S&#34;) + &#34;\n&#34;
    print(msg, flush=True)</code></pre>
</details>
</dd>
<dt id="TIEGCM_Statistics.Manager.StartCalculating"><code class="name flex">
<span>def <span class="ident">StartCalculating</span></span>(<span>NetCDF_files_path, ResultFilename, TypeOfCalculation, TmpFilesPath='/media/balukid/STATStmp/')</span>
</code></dt>
<dd>
<div class="desc"><p>This function manages the statistical calulation:<br>
- For each TIE-GCM netCDF file it spawns a process which will store its results into temporary files at its own folder. Many procecess are utilised in order to accelerate the calculation by leveraging multiple cores.<br>
- After all processes finish, this function merges all temporary files, calculates percentiles etc for each bin and creates a results netCDF file.<br>
This function should be called after the Data.setDataParams() function has initialized the bin ranges.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>NetCDF_files_path</code></strong> :&ensp;<code>string</code></dt>
<dd>The path where all the TIE-GCM netCDF files are stored. In wildcard format. Example: "./data/<em>/</em>.nc".</dd>
<dt><strong><code>ResultFilename</code></strong> :&ensp;<code>string</code></dt>
<dd>The filename where the final calculation results will be stored.</dd>
<dt><strong><code>TypeOfCalculation</code></strong> :&ensp;<code>string</code></dt>
<dd>The variable which upon which the calculation will be applied. See Data.setDataParams() for more.</dd>
<dt><strong><code>TmpFilesPath</code></strong> :&ensp;<code>string</code></dt>
<dd>The path where the temporary files will be stores</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def StartCalculating( NetCDF_files_path, ResultFilename, TypeOfCalculation, TmpFilesPath=&#34;/media/balukid/STATStmp/&#34; ):
    &#39;&#39;&#39;
    This function manages the statistical calulation:  
      - For each TIE-GCM netCDF file it spawns a process which will store its results into temporary files at its own folder. Many procecess are utilised in order to accelerate the calculation by leveraging multiple cores.  
      - After all processes finish, this function merges all temporary files, calculates percentiles etc for each bin and creates a results netCDF file.  
    This function should be called after the Data.setDataParams() function has initialized the bin ranges.
    Args:
        NetCDF_files_path (string): The path where all the TIE-GCM netCDF files are stored. In wildcard format. Example: &#34;./data/*/*.nc&#34;.
        ResultFilename (string): The filename where the final calculation results will be stored.
        TypeOfCalculation (string): The variable which upon which the calculation will be applied. See Data.setDataParams() for more.
        TmpFilesPath (string): The path where the temporary files will be stores
    &#39;&#39;&#39;
    global TMP_FOLDER
    global theResultFile_folder
    global theResultFile_simplename
    theResultFile_folder   = ResultFilename[ 0 : ResultFilename.rfind(&#39;/&#39;)  ]
    theResultFile_simplename = ResultFilename[ ResultFilename.rfind(&#39;/&#39;)+1 :  ][0:-3]
    if not os.path.exists(theResultFile_folder+&#34;/&#34;+theResultFile_simplename): os.makedirs(theResultFile_folder+&#34;/&#34;+theResultFile_simplename)
    
    startSecs = time.time()
    print( &#34;START&#34;, datetime.datetime.now().strftime(&#34;%d-%m-%Y %H:%M:%S&#34;) )
    TMP_FOLDER = TmpFilesPath
    
    Allprocesses = list()
    AllCDFfiles = sorted( glob.glob( NetCDF_files_path, recursive=True ) )
    print( &#34;I will calculate &#39;&#34; + TypeOfCalculation + &#34;&#39; on&#34;, len(AllCDFfiles), &#34;files:\n    &#34;, NetCDF_files_path, &#34;\n&#34; )
    print( &#34;Results will be stored in &#39;&#34; + ResultFilename + &#34;&#39;\n&#34; )
    
    n = 0
    for CDF_file in AllCDFfiles:
        n += 1
        Data.Progress = int( 100 * n/221)

        # spawn new process
        P = multiprocessing.Process(target=PROC_StatsCalculator, args=(n,CDF_file,TypeOfCalculation))
        Allprocesses.append(P)
        P.start()

        pause_spawning = True
        while pause_spawning:
            Num_of_alive_processes = 0
            for P in Allprocesses:
                if P.is_alive():
                    Num_of_alive_processes += 1            
            if Num_of_alive_processes &gt;= NUM_OF_PROCESSORS:
                pause_spawning = True
                time.sleep(12)
            else:
                pause_spawning = False


        # wait for all processes to terminate
        for T in Allprocesses: T.join()
        
    # every process creates a partial file, merge all of them into one
    print( &#34;Merging partial data files and calculating result values...&#34;,  datetime.datetime.now().strftime(&#34;%d-%m-%Y %H:%M:%S&#34;))
    ResultBuckets = Data.init_ResultDataStructure()
    NumOfBins = len(Data.KPsequence) * len(Data.ALTsequence) * len(Data.LATsequence) * len(Data.MLTsequence)
    CurrBinNum = 0
    
    print( &#34;Data.KPsequence: &#34;, Data.KPsequence)
    print( &#34;Data.ALTsequence&#34;, Data.ALTsequence)
    print( &#34;Data.LATsequence&#34;, Data.LATsequence)
    print( &#34;Data.MLTsequence&#34;, Data.MLTsequence)
    
    for aKP in Data.KPsequence:
        for aMLT in Data.MLTsequence:
            RegionHits = 0
            for anALT in Data.ALTsequence:
                for aLat in Data.LATsequence:
                    CurrBinNum += 1
                    Data.Progress = int( 100 * CurrBinNum/NumOfBins )
                    AllBinValues = list()
                    All_partialData_folders = sorted( glob.glob( TMP_FOLDER+&#34;proc*&#34;, recursive=False ) )
                    for partialDataFolder in All_partialData_folders: # read all partial files for this bin 
                        partialDataFolder = partialDataFolder + &#34;/&#34;
                        if os.path.isdir(partialDataFolder)==False:
                            continue
                        partialDataFilename = partialDataFolder + str(aKP)+&#34;_&#34;+str(anALT)+&#34;_&#34;+str(aLat)+&#34;_&#34;+str(aMLT)+&#34;.dat&#34;
                        if os.path.exists(partialDataFilename) == False: # no hits for this bin from this process
                            continue
                            
                        f = open(partialDataFilename, &#34;rb&#34;)
                        float_array = array(&#39;d&#39;)
                        float_array.frombytes(f.read())
                        AllBinValues += float_array.tolist()
                        f.close()
                        
                    print(&#34;BIN&#34;, &#34;Kp&#34;+str(aKP), &#34;Alt&#34;+str(anALT), &#34;Lat&#34;+str(aLat), &#34;MLT&#34;+str(aMLT), &#34;&#34;, len(AllBinValues), &#34;items&#34; )
                    RegionHits += len(AllBinValues)
                        
                    if len(AllBinValues) &gt; 0:
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Sum&#34;] = np.sum(AllBinValues)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Len&#34;] = len(AllBinValues)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile10&#34;] = np.percentile(AllBinValues, 10)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile25&#34;] = np.percentile(AllBinValues, 25)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile50&#34;] = np.percentile(AllBinValues, 50)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile75&#34;] = np.percentile(AllBinValues, 75)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Percentile90&#34;] = np.percentile(AllBinValues, 90)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Variance&#34;] = np.var(AllBinValues)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Minimum&#34;] = np.nanmin(AllBinValues)
                        ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Maximum&#34;] = np.nanmax(AllBinValues)
                        
                        # calculate distribution
                        if Data.DistributionNumOfSlots &gt; 0:
                            histo_values, histo_ranges = np.histogram(AllBinValues, Data.DistributionNumOfSlots, (0, 0.0000001))
                            for i in range(0, Data.DistributionNumOfSlots):
                                ResultBuckets[aKP, anALT, aLat, aMLT, &#34;Distribution&#34;][i] = histo_values[i]
            print( &#34;REGION:&#34;, &#34;Kp&#34;+str(aKP), &#34;MLT&#34;+str(aMLT), &#34;:&#34;, RegionHits, &#34;measurements.&#34; )
            print(&#34;BIN&#34;, &#34;Kp&#34;+str(aKP), &#34;Alt&#34;+str(anALT), &#34;Lat&#34;+str(aLat), &#34;MLT&#34;+str(aMLT), &#34;&#34;, len(AllBinValues), &#34;items&#34; )
            
    if &#34;Ohmic&#34; in TypeOfCalculation:
        Data.WriteResultsToCDF(ResultBuckets, ResultFilename, &#34;Joule Heating&#34;, &#34;W/m3&#34;)
    elif &#34;SIGMA_PED&#34; in TypeOfCalculation:
        Data.WriteResultsToCDF(ResultBuckets, ResultFilename, &#34;Pedersen Conductivity&#34;, &#34;S/m&#34;)
    else:
        Data.WriteResultsToCDF(ResultBuckets, ResultFilename, TypeOfCalculation, &#34;&#34;)
    
    # DO NOT DELL PARTIAL FILES - THEY CAN BE USED TO CONTINUE THE CALCULATION AFTER AN INTERMEDIATE HALT
    #try: # delete temporary files, which contain all values for each bin
    #    shutil.rmtree( TMP_FOLDER )
    #except:
    #    pass
    
    finishSecs = time.time()
    print( &#34;FINISH&#34;,  datetime.datetime.now().strftime(&#34;%d-%m-%Y %H:%M:%S&#34;), &#34; (&#34;, finishSecs-startSecs, &#34;sec )&#34;)</code></pre>
</details>
</dd>
<dt id="TIEGCM_Statistics.Manager.Store_NumOfMeasurements"><code class="name flex">
<span>def <span class="ident">Store_NumOfMeasurements</span></span>(<span>aKP, aMLT, curr_num)</span>
</code></dt>
<dd>
<div class="desc"><p>Stores the number of measurements that fall inside each sub-region defined by its lower Kp limit and lower MLT limit.
There is one file per sub-region.
This function is necessary because the data are processed by different processes.</p>
<h2 id="args">Args</h2>
<p>aKP (float):
aMLT (float):
curr_num (int):</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Store_NumOfMeasurements(aKP, aMLT, curr_num):
    &#39;&#39;&#39;
    Stores the number of measurements that fall inside each sub-region defined by its lower Kp limit and lower MLT limit. 
    There is one file per sub-region.
    This function is necessary because the data are processed by different processes.
    Args:
        aKP (float):
        aMLT (float):
        curr_num (int):
    &#39;&#39;&#39;
    # LOCK
    wait = True
    while( wait ):
        if os.path.exists( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + &#34;lock.tmp&#34; ) == False: 
            try:
                f = open( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + &#34;lock.tmp&#34;, &#34;x&#34; )
                f.close()
                wait = False
            except:
                wait = True
        if wait: time.sleep( random.randint(0,1) )
    # WORK
    prev_num = 0
    fname = str(aKP) + &#34;_&#34; + str(aMLT) + &#34;.txt&#34;
    if os.path.exists( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + fname ):
        f = open( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + fname, &#34;r&#34; )
        prev_num = int( f.read() )
        f.close()
    f = open( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + fname, &#34;w&#34; )
    f.write( str(prev_num + curr_num) )
    f.close()
    # UNLOCK
    try:
        os.remove( theResultFile_folder + &#34;/&#34; + theResultFile_simplename + &#34;/&#34; + &#34;lock.tmp&#34; )
    except:
        pass</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TIEGCM_Statistics" href="index.html">TIEGCM_Statistics</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="TIEGCM_Statistics.Manager.PROC_StatsCalculator" href="#TIEGCM_Statistics.Manager.PROC_StatsCalculator">PROC_StatsCalculator</a></code></li>
<li><code><a title="TIEGCM_Statistics.Manager.StartCalculating" href="#TIEGCM_Statistics.Manager.StartCalculating">StartCalculating</a></code></li>
<li><code><a title="TIEGCM_Statistics.Manager.Store_NumOfMeasurements" href="#TIEGCM_Statistics.Manager.Store_NumOfMeasurements">Store_NumOfMeasurements</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>